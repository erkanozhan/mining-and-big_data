***

Merhaba gençler,

Bugün sizlerle son yılların en popüler konularından biri olan **Büyük Veri (Big Data)** üzerine konuşacağız. Bu kavramı muhtemelen duymuşsunuzdur ama tam olarak ne anlama geldiğini, neden bu kadar önemli olduğunu ve arkasındaki teknolojileri adım adım inceleyeceğiz.

### Büyük Veri Nedir?

En basit haliyle başlayalım. Elinizde tek bir kitaptan oluşan bir bilgi olduğunu düşünün. İçindeki bilgileri bulmak, analiz etmek oldukça kolaydır. Şimdi, o tek kitap yerine dünyanın en büyük kütüphanesindeki tüm kitapların, dergilerin, ses kayıtlarının ve videoların bir anda önünüze yığıldığını hayal edin. Üstelik bu kütüphaneye her saniye binlerce yeni materyal ekleniyor. İşte bu devasa, karmaşık ve sürekli büyüyen bilgi yığınına **Büyük Veri** diyoruz.

Geleneksel yöntemlerimiz, yani standart bir bilgisayar ve basit programlar, bu kütüphaneyi anlamlandırmak için yetersiz kalır. Bir soru sorduğunuzda cevabı saatler, hatta günler sürebilir. Büyük Veri alanı, tam da bu sorunu çözmek için var. Yani, birbirinden farklı kaynaklardan gelen (sosyal medya, sensörler, web siteleri, banka işlemleri vb.) devasa veri koleksiyonlarını;

*   Analiz etmeye,
*   İşlemeye,
*   ve Depolamaya adanmış bir alandır.

Büyük Veri'nin temel amacı, bu veri yığınının içindeki **değeri**, yani **anlamı** ortaya çıkarmaktır. Tıpkı bir madencinin tonlarca toprağı eleyerek değerli madenleri bulması gibi, biz de Büyük Veri'yi işleyerek değerli bilgilere ve öngörülere ulaşırız.

### Büyük Veri'nin Getirdiği Kazanımlar

Peki, bu devasa veriyi işlediğimizde elimize ne geçiyor? Elde ettiğimiz sonuçlar, kurumlar için çok çeşitli öngörülere ve kazanımlara yol açabilir. Örneğin:

*   **Operasyonel Optimizasyon:** Bir fabrikanın üretim hattındaki sensör verilerini analiz ederek arızaları önceden tespit edebilir ve üretimi daha verimli hale getirebiliriz.
*   **Eyleme Geçirilebilir Bilgi:** Müşteri davranışlarını analiz ederek onlara en uygun ürünleri, doğru zamanda sunabiliriz.
*   **Yeni Pazarların Tanımlanması:** Toplumdaki yeni eğilimleri ve ihtiyaçları herkesten önce fark ederek yeni iş fırsatları yaratabiliriz.
*   **Doğru Tahminler:** Hava durumu tahminlerinden borsa hareketlerine kadar, geçmiş verileri analiz ederek geleceğe dair daha isabetli öngörülerde bulunabiliriz.
*   **Hata ve Sahtekarlık Tespiti:** Bankacılıkta, anormal işlem desenlerini anında tespit ederek sahtekarlığı önleyebiliriz.
*   **Geliştirilmiş Karar Verme:** Sezgilere veya sınırlı bilgilere dayanmak yerine, somut verilere dayalı daha sağlam ve doğru kararlar alabiliriz.
*   **Bilimsel Keşifler:** Genom verilerinin analizinden uzay araştırmalarına kadar, bilim dünyasında çığır açan keşiflere imkan tanır.

### Veri Analitiği: Veriyi Anlamlandırma Sanatı

Büyük Veri'yi topladık, peki onu nasıl anlamlı hale getireceğiz? İşte burada **Veri Analitiği (Data Analytics)** devreye giriyor. Veri analitiği, ham veriden anlamlı sonuçlar çıkarmak için kullandığımız yöntemlerin, tekniklerin ve araçların tümünü kapsayan geniş bir disiplindir.

Bu analiz sürecini dört ana kategoriye ayırabiliriz. Bunu bir doktorun hasta teşhisi gibi düşünebilirsiniz:

1.  **Açıklayıcı Analitik (Descriptive Analytics): "Ne Oldu?"**
    Bu en temel analiz türüdür. Geçmişte ne olduğunu özetler.
    *   *Doktorun teşhisi:* "Hastanın ateşi 39 derece."
    *   *İş dünyasından örnekler:* "Geçen ay ne kadar satış yaptık?", "Hangi bölgeden daha çok destek talebi geldi?"

2.  **Tanısal Analitik (Diagnostic Analytics): "Neden Oldu?"**
    Olayların arkasındaki nedenleri bulmaya odaklanır.
    *   *Doktorun teşhisi:* "Kan tahlili sonuçlarına göre bu yüksek ateşin sebebi bakteriyel bir enfeksiyon."
    *   *İş dünyasından örnekler:* "Neden ikinci çeyrek satışları ilk çeyrekten daha düşüktü?", "Doğu bölgesindeki destek çağrıları neden Batı'dan fazlaydı?"

3.  **Tahmine Dayalı Analitik (Predictive Analytics): "Ne Olacak?"**
    Geçmiş verilerdeki desenleri kullanarak gelecekte ne olabileceğini tahmin eder.
    *   *Doktorun teşhisi:* "Bu enfeksiyon türü genellikle 3 gün içinde ilaç tedavisiyle kontrol altına alınır."
    *   *İş dünyasından örnekler:* "Bir müşterinin kredi borcunu ödememe olasılığı nedir?", "Bu reklam kampanyası yürütülürse satışlar ne kadar artar?"

4.  **Yönetsel (Normatif) Analitik (Prescriptive Analytics): "Ne Yapmalıyız?"**
    En gelişmiş analiz türüdür. Sadece ne olacağını söylemekle kalmaz, en iyi sonuca ulaşmak için ne yapılması gerektiğini de önerir.
    *   *Doktorun teşhisi:* "En iyi sonuç için bu antibiyotiği günde iki kez almalısınız."
    *   *İş dünyasından örnekler:* "Talebi karşılamak için hangi depodan hangi mağazaya ne kadar ürün göndermeliyiz?", "Kârı maksimize etmek için hangi ürünlere indirim yapmalıyız?"

### Büyük Veri'nin 5 Temel Özelliği (5V)

Bir veri setinin "Büyük Veri" olarak adlandırılabilmesi için genellikle "V" harfiyle başlayan beş temel özelliğe sahip olması beklenir.

1.  **Hacim (Volume):** Veri miktarının çok büyük olmasıdır. Gigabaytlar, terabaytlar, hatta petabaytlar seviyesinde veri söz konusudur. Örneğin, Facebook her gün yüz milyonlarca fotoğraf ve video yüklüyor.

2.  **Hız (Velocity):** Verinin ne kadar hızlı üretildiğini ve işlenmesi gerektiğini belirtir. Bir yangın musluğundan akan su gibi düşünebilirsiniz. Örneğin, borsadaki anlık işlemler, bir jet motorundan gelen sensör verileri veya saniyede atılan binlerce tweet.

3.  **Çeşitlilik (Variety):** Verinin farklı türlerde ve formatlarda olmasını ifade eder. Sadece sayılardan oluşan düzenli tablolar (yapılandırılmış veri) değil, aynı zamanda metinler, e-postalar, videolar, ses kayıtları, fotoğraflar (yapılandırılmamış veri) ve XML/JSON dosyaları (yarı yapılandırılmış veri) gibi çok çeşitli formatları içerir.

4.  **Doğruluk (Veracity):** Verinin kalitesini ve güvenilirliğini temsil eder. İnternetteki her bilgi doğru değildir, değil mi? Veri setleri de "gürültü" içerebilir; yani eksik, hatalı veya tutarsız bilgiler barındırabilir. Analizden önce bu veriyi temizlemek ve doğruluğundan emin olmak çok önemlidir.

5.  **Değer (Value):** En önemli özellik budur. Eğer işlediğimiz veri, bize bir fayda sağlamıyor, bir sorunu çözmüyor veya bir karar almamıza yardımcı olmuyorsa, o veriyi toplamanın ve işlemenin bir anlamı yoktur. Verinin değeri, ne kadar doğru olduğu ve ne kadar hızlı işlenip anlamlı bir sonuca dönüştürüldüğü ile doğrudan ilişkilidir.

## Büyük Veri Depolama ve İşleme: Hadoop'a Giriş

Peki, bu devasa ve karmaşık veriyi nerede ve nasıl işleyeceğiz? Tek bir süper bilgisayar bile bu yükün altından kalkamaz. Çözüm, gücü bölmek ve dağıtmaktır. İşte burada **kümeler (clusters)** ve **dağıtılmış sistemler** devreye giriyor.

*   **Küme (Cluster):** Birbirine hızlı bir ağ ile bağlı çok sayıda standart bilgisayarın (bunlara *düğüm* veya *node* denir) tek bir sistem gibi çalışmasıdır. Görev, küçük parçalara bölünür ve her bir bilgisayar görevin bir parçasını üstlenir.
*   **Dağıtılmış Dosya Sistemi:** Büyük bir dosyayı tek bir bilgisayarda saklamak yerine, onu küçük parçalara (*bloklara*) bölerek kümedeki farklı bilgisayarlara dağıtan bir sistemdir. Böylece hem depolama kapasitesi artar hem de veriye aynı anda birden çok bilgisayar erişebilir.

Peki Hadoop tam olarak nedir? Şöyle düşünelim: Çok büyük bir yapbozu tek başınıza tamamlamanız haftalar sürebilir. Ama aynı yapbozu 100 arkadaşınıza dağıtırsanız, her biri kendi küçük parçasını yapar ve sonra bu parçaları birleştirerek yapbozu çok daha hızlı tamamlarsınız. Hadoop, tam olarak bu mantıkla çalışır; devasa bir veri işleme görevini, standart donanımlara sahip yüzlerce, hatta binlerce bilgisayardan oluşan bir kümeye dağıtır ve paralel olarak çözmelerini sağlar.

Daha yapısal bir bakışla, Hadoop'u dört ana bileşenden oluşan bir çerçeve olarak tanımlayabiliriz, bunlar:

![Büyük Veri Mimarisi](images/svgviewer-output.svg)

1.  **HDFS (Hadoop Distributed File System):** Hadoop'un dağıtılmış depolama birimidir. Büyük dosyaları *blok* adı verilen parçalara ayırır ve kümedeki farklı makinelere dağıtır. Veri kaybını önlemek için her bloğun kopyalarını oluşturur ve farklı makinelere yedekler. Bu işleme *replikasyon* denir ve sistemin hataya karşı dayanıklı olmasını sağlar.

    ![HDFS Mimarisi](images/HDFS.svg)

2.  **YARN (Yet Another Resource Negotiator):** Kümenin kaynak yöneticisidir. Hangi işin hangi makinede çalışacağını planlar, işlem gücü (CPU) ve bellek (RAM) gibi kaynakları işler arasında adil bir şekilde dağıtır. Kısacası kümenin işletim sistemi gibi davranır. Detaylarına birazdan değineceğiz.

3.  **MapReduce:** Büyük veri işleme için kullanılan bir programlama modelidir. Temelde iki adımdan oluşur:
    *   **Map:** Büyük bir görevi alır ve onu kümedeki tüm makinelere dağıtılacak küçük, paralel görevlere böler. (Örneğin, milyonlarca belgedeki kelimeleri sayma görevini, her makinenin kendi üzerindeki birkaç belgeyi sayması şeklinde bölmek).
    *   **Reduce:** Map aşamasından gelen tüm kısmi sonuçları toplar ve tek bir nihai sonuçta birleştirir. (Örneğin, tüm makinelerden gelen kelime sayılarını toplayarak genel toplamı bulmak).

4.  **Hadoop Common:** Diğer Hadoop modüllerinin çalışması için gerekli olan ortak kütüphaneleri ve yardımcı programları içerir.

Bu yapı sayesinde Hadoop, hem çok büyük verileri uygun maliyetli bir şekilde depolayabilir hem de bu veriyi paralel olarak çok hızlı bir şekilde işleyebilir. Önce tek bir makine üzerinde Hadoop'u nasıl çalıştıracağımızı, ardından da bunu küçük bir kümeye nasıl dönüştüreceğimizi göreceğiz.

# Hadoop: Dağıtık Sistemlere Giriş

## Büyük Veri Nedir ve Neden Dağıtık Sistemlere İhtiyaç Duyarız?

Gençler tekrar hatırlayalım, düşünün ki elinizde 1 TB boyutunda bir metin dosyası var ve bu dosyadaki her kelimenin kaç kez geçtiğini saymak istiyorsunuz. Normal bir bilgisayarda bu işlem saatler, belki de günler sürebilir. Peki ya 100 bilgisayar aynı anda bu dosyanın farklı parçaları üzerinde çalışsa? İşte dağıtık sistemlerin temel mantığı budur: büyük bir problemi küçük parçalara böl, her parçayı farklı bir makinede işle, sonuçları birleştir.

Hadoop, bu fikri gerçeğe dönüştüren bir yazılım çerçevesidir. İki temel bileşenden oluşur:

1. **HDFS (Hadoop Distributed File System):** Verileri birden fazla makineye dağıtarak saklar
2. **YARN (Yet Another Resource Negotiator):** İşlem kaynaklarını yönetir ve görevleri dağıtır

## Hadoop Mimarisi: Kim Ne Yapar?

Bir Hadoop kümesi, tıpkı bir şirketteki organizasyon yapısı gibi çalışır. Bir yönetici (Master) ve birden fazla çalışan (Worker) vardır.

### Master Düğümde Çalışan Servisler

**NameNode:** HDFS'in beynidir. Hangi dosyanın hangi parçalarının (blok) nerede saklandığını bilir. Dosya sistemi ağacını, blok konumlarını ve metadata bilgilerini tutar. Ancak verilerin kendisini saklamaz; sadece "haritayı" tutar.

**ResourceManager:** YARN'ın merkezidir. Kümedeki tüm hesaplama kaynaklarını (CPU, bellek) yönetir. Bir iş geldiğinde, bu işi hangi makinelerde çalıştıracağına karar verir ve kaynakları tahsis eder.

**SecondaryNameNode:** Adına aldanmayın, bu bir yedek NameNode değildir. NameNode'un edit log dosyalarını periyodik olarak birleştirerek checkpoint oluşturur. Bu sayede NameNode'un başlangıç süresi kısalır ve kurtarma işlemleri kolaylaşır.

### Worker Düğümlerde Çalışan Servisler

**DataNode:** Gerçek verileri saklayan servisdir. Büyük dosyalar 128 MB'lık bloklara bölünür ve bu bloklar farklı DataNode'lara dağıtılır. Her DataNode, sakladığı blokların listesini periyodik olarak NameNode'a bildirir (heartbeat).

**NodeManager:** Her worker makinedeki kaynak yöneticisidir. ResourceManager'dan gelen taleplere göre container'lar oluşturur ve bu container'larda Map veya Reduce görevlerini çalıştırır.

## Pseudo-Distributed Mod: Tek Makinede Tam Deneyim

Gerçek bir küme kurmadan önce, tüm bu servisleri tek bir makine üzerinde çalıştırarak sistemi öğrenebiliriz. Bu moda "Pseudo-Distributed" (sözde-dağıtık) denir. Tüm servisler aynı makinede çalışır, ancak gerçek bir küme gibi davranır.

### Gereksinimler

Hadoop, Java Virtual Machine üzerinde çalışır. Bu nedenle önce Java'yı kurmamız gerekir. Ayrıca Hadoop, düğümler arası iletişim için SSH protokolünü kullanır.

```bash
# Sistem güncellemesi
sudo apt update && sudo apt upgrade -y

# Java kurulumu
sudo apt install openjdk-11-jdk -y

# SSH kurulumu
sudo apt install ssh openssh-server -y

# Yüklü Linux'ü görme	
lsb_release -a

# Java Versionu Görme	
java -version

# Javanın yüklendiği yeri görme
readlink -f $(which java)

# Hangi javalar yüklü hangisi aktif ve nerede 
update-java-alternatives -l

#İstenilen Java Sürümünü Aktif Etme	
sudo update-alternatives --config java
```

Java kurulumunu doğrulamak için `java -version` komutunu çalıştırabilirsiniz.

### Hadoop Kullanıcısı Oluşturma

Sistem servislerini ayrı bir kullanıcı altında çalıştırmak, güvenlik ve yönetim açısından iyi bir uygulamadır.

```bash
# Grup ve kullanıcı oluşturma
sudo addgroup hadoop
sudo adduser --ingroup hadoop hduser

# Yeni kullanıcıya geçiş
su - hduser
```

### SSH Yapılandırması

Hadoop servisleri başlatıldığında, script'ler SSH üzerinden ilgili düğümlere bağlanır. Tek makinede bile olsak, `hduser` kullanıcısının kendi kendine parola sormadan SSH yapabilmesi gerekir.

```bash
# Anahtar çifti oluşturma
ssh-keygen -t rsa -P '' -f ~/.ssh/id_rsa

# Açık anahtarı yetkili anahtarlara ekleme
cat ~/.ssh/id_rsa.pub >> ~/.ssh/authorized_keys
chmod 0600 ~/.ssh/authorized_keys
```

Test için `ssh localhost` komutunu çalıştırdığınızda parola sorulmamalıdır.

### Hadoop İndirme ve Kurulum

Apache Hadoop'un resmi dağıtımını indirip uygun bir konuma yerleştireceğiz.

```bash
# İndirme
wget https://dlcdn.apache.org/hadoop/common/hadoop-3.3.6/hadoop-3.3.6.tar.gz

# Arşivi açma ve taşıma
tar xzf hadoop-3.3.6.tar.gz
sudo mv hadoop-3.3.6 /usr/local/hadoop

# Sahiplik ayarı
sudo chown -R hduser:hadoop /usr/local/hadoop
```

### Ortam Değişkenleri

Hadoop komutlarının sistemde her yerden çalışabilmesi için PATH değişkenini ve Hadoop'un ihtiyaç duyduğu ortam değişkenlerini tanımlamamız gerekir. `~/.bashrc` dosyasının sonuna şu satırları ekleyin:

```bash
export JAVA_HOME=/usr/lib/jvm/java-11-openjdk-amd64
export HADOOP_HOME=/usr/local/hadoop
export PATH=$PATH:$HADOOP_HOME/bin:$HADOOP_HOME/sbin
```

Değişiklikleri aktif etmek için `source ~/.bashrc` komutunu çalıştırın.

## Yapılandırma Dosyaları

Hadoop'un davranışını belirleyen yapılandırma dosyaları `$HADOOP_HOME/etc/hadoop` dizinindedir. Her dosya belirli bir bileşeni yapılandırır.

### hadoop-env.sh

Bu dosya, Hadoop'un hangi Java kurulumunu kullanacağını belirtir.

```bash
export JAVA_HOME=/usr/lib/jvm/java-11-openjdk-amd64
```

### core-site.xml

HDFS için varsayılan dosya sistemini tanımlar. `fs.defaultFS` özelliği, NameNode'un adresini belirtir.

```xml
<configuration>
    <property>
        <name>fs.defaultFS</name>
        <value>hdfs://localhost:9000</value>
    </property>
</configuration>
```

### hdfs-site.xml

HDFS'e özgü ayarları içerir. Önce veri dizinlerini oluşturun:

```bash
mkdir -p ~/hadoop_data/hdfs/namenode
mkdir -p ~/hadoop_data/hdfs/datanode
```

Yapılandırma:

```xml
<configuration>
    <property>
        <name>dfs.replication</name>
        <value>1</value>
    </property>
    <property>
        <name>dfs.namenode.name.dir</name>
        <value>file:///home/hduser/hadoop_data/hdfs/namenode</value>
    </property>
    <property>
        <name>dfs.datanode.data.dir</name>
        <value>file:///home/hduser/hadoop_data/hdfs/datanode</value>
    </property>
</configuration>
```

`dfs.replication` değeri, her veri bloğunun kaç kopyasının tutulacağını belirtir. Tek makinede olduğumuz için bu değer 1'dir. Gerçek kümelerde genellikle 3 kullanılır.

### mapred-site.xml

MapReduce işlerinin hangi çerçeve üzerinde çalışacağını belirtir.

```xml
<configuration>
    <property>
        <name>mapreduce.framework.name</name>
        <value>yarn</value>
    </property>
</configuration>
```

### yarn-site.xml

YARN için temel ayarları içerir.

```xml
<configuration>
    <property>
        <name>yarn.nodemanager.aux-services</name>
        <value>mapreduce_shuffle</value>
    </property>
</configuration>
```

`mapreduce_shuffle` ayarı, Map ve Reduce aşamaları arasındaki veri transferini (shuffle) etkinleştirir.

## HDFS Formatlama ve Servisleri Başlatma

HDFS'i ilk kez kullanmadan önce formatlanması gerekir. Bu işlem, NameNode için gerekli metadata yapısını oluşturur.

```bash
hdfs namenode -format
```

Bu komutu yalnızca ilk kurulumda çalıştırın. Çalışan bir kümede bu komutu çalıştırmak tüm verileri siler.

Servisleri başlatmak için:

```bash
start-dfs.sh
start-yarn.sh
```

Çalışan Java süreçlerini görmek için `jps` komutunu kullanın. Çıktıda şunları görmelisiniz: NameNode, DataNode, SecondaryNameNode, ResourceManager, NodeManager.

### Web Arayüzleri

Hadoop, kümenin durumunu izlemek için web arayüzleri sunar:

- **HDFS NameNode:** http://localhost:9870
- **YARN ResourceManager:** http://localhost:8088

Bu arayüzlerde "Live Nodes" sayısının 1 olduğunu görüyorsanız kurulum başarılıdır.

## Çok Düğümlü Kümeye Geçiş

Tek düğümlü kurulumun mantığını kavradıktan sonra, gerçek dağıtık ortama geçiş yapmak oldukça kolaydır. İki makinelik minimal bir küme için yapılması gereken değişiklikler şunlardır:

### Master-Worker İletişimi

Master ve Worker makineler arasında `hduser` kullanıcısı için parolasız SSH yapılandırması gerekir. Master makineden `ssh worker-node` komutu parola sormadan çalışmalıdır.

### Yapılandırma Değişiklikleri

**core-site.xml:** `localhost` yerine Master düğümün IP adresi veya hostname'i yazılır.

```xml
<property>
    <name>fs.defaultFS</name>
    <value>hdfs://master-node:9000</value>
</property>
```

**yarn-site.xml:** ResourceManager'ın adresini ekleyin.

```xml
<property>
    <name>yarn.resourcemanager.hostname</name>
    <value>master-node</value>
</property>
```

**workers dosyası:** Worker düğümlerin listesini içerir. `localhost` satırını silip worker hostname'lerini ekleyin.

```
worker-node
```

### Yapılandırmayı Dağıtma

Master'daki yapılandırmayı tüm worker'lara kopyalamanız gerekir:

```bash
# Worker'da dizin oluşturma
ssh hduser@worker-node 'mkdir -p ~/hadoop_data/hdfs/datanode'

# Yapılandırma dosyalarını kopyalama
scp -r $HADOOP_HOME/etc/hadoop hduser@worker-node:$HADOOP_HOME/etc/
```

Artık `start-dfs.sh` ve `start-yarn.sh` komutları, workers dosyasındaki tüm makinelere SSH ile bağlanarak ilgili servisleri başlatacaktır.

## MapReduce: İlk İş

Kurulumu test etmek için Hadoop ile gelen WordCount örneğini çalıştıralım.

```bash
# HDFS'te girdi dizini oluşturma
hdfs dfs -mkdir /input

# Test dosyası hazırlama ve yükleme
echo "merhaba dunya hosgeldin dunya" > test.txt
hdfs dfs -put test.txt /input

# MapReduce işini çalıştırma
hadoop jar $HADOOP_HOME/share/hadoop/mapreduce/hadoop-mapreduce-examples-*.jar \
    wordcount /input /output

# Sonuçları görüntüleme
hdfs dfs -cat /output/part-r-00000
```

Beklenen çıktı:

```
dunya       2
hosgeldin   1
merhaba     1
```

## Kümeyi Durdurma

İşiniz bittiğinde servisleri düzgün bir şekilde kapatın:

```bash
stop-yarn.sh
stop-dfs.sh
```

## Sorun Giderme İpuçları

Log dosyaları `$HADOOP_HOME/logs` dizininde bulunur. Bir servis başlamıyorsa, ilgili log dosyasını inceleyerek hatanın kaynağını tespit edebilirsiniz.

Sık karşılaşılan sorunlar:

- **SSH bağlantı hatası:** Parolasız SSH yapılandırmasını kontrol edin
- **NameNode formatlanamıyor:** Daha önceki veri dizinlerini temizleyin
- **DataNode bağlanamıyor:** Firewall ayarlarını ve port erişimlerini kontrol edin

**Firewall Ayarları ve Yönetimi
```bash
# Firewall'ı açma
sudo ufw enable
#Port Açma
sudo ufw allow 8080
#Protokol Bazlı Port Açma
sudo ufw allow 8080/tcp
sudo ufw allow 8080/udp
# Firewall'ın Durumunu (Açıkmı-Kapalımı?) Görme
sudo ufw status
```

- **Java bulunamıyor:** JAVA_HOME değişkeninin doğru tanımlandığından emin olun

Hata mesajlarını dikkatlice okumak, dağıtık sistemlerde sorun çözme becerinizi geliştirecek en önemli alışkanlıktır.

***
## Apache Spark, Hive, Pig ve Mahout: Hadoop Ekosisteminin Diğer Oyuncuları
Hadoop’un temel yapısını, yani veriyi nasıl depoladığını (HDFS) ve bu veriyi nasıl işlediğini (MapReduce) geçen haftalarda konuştuk. Ancak Hadoop saf haliyle, yani sadece Java kodları yazarak veri işlemek, özellikle büyük ölçekli projelerde zaman alıcı ve zahmetli olabilir. Bu sebeple ekosistemde işlerimizi kolaylaştıran, veriye erişimi hızlandıran ve analiz yeteneklerimizi artıran başka araçlar geliştirilmiştir.

Bugün, bu ekosistemin diğer önemli oyuncularını; Spark, Hive, Pig ve Mahout’u konuşacağız. Ayrıca veri akışını sağlayan Sqoop ve Flume gibi araçlara da kısaca değinmekte fayda var.

Gençler, bu araçları anlamak için öncelikle neden var olduklarını kavramamız gerekiyor.

Hadoop’u devasa bir kütüphane deposu gibi düşünün. Milyonlarca kitap var ama aradığınızı bulmak veya bu kitaplardan bir özet çıkarmak için her seferinde depoya girip tek tek rafları gezmeniz, kitapları indirmeniz ve not almanız gerekiyor. Bu, MapReduce’un çalışma mantığıdır; güçlüdür ama yavaştır.

**Apache Hive**, bu depoda çalışan ve sizin dilinizden anlayan bir kütüphane memuru gibidir. Siz ona "Bana tarihi romanların listesini getir" dersiniz (ki buna SQL benzeri bir dil diyoruz), o arka planda gidip o zorlu rafları tarar ve sonucu size getirir. Sizi karmaşık Java kodları yazmaktan kurtarır, sanki standart bir veritabanı kullanıyormuşsunuz gibi hissettirir.

**Apache Pig** ise daha çok veri üzerinde temizlik ve düzenleme yapan bir mutfak şefi gibidir. Veri çok dağınık, düzensiz veya kirli olabilir. Pig, bu veriyi alır, parçalar, filtreler ve istediğiniz formata sokar. Karmaşık bir iş akışını adım adım planlamanızı sağlar. "Önce domatesleri yıka, sonra doğra, en son pişir" gibi prosedürel bir dille veriyi işlersiniz.

**Apache Spark**’a gelince, işin rengi burada biraz değişiyor. Hadoop veriyi işlerken sürekli diske yazar ve diskten okur. Bu da vakit kaybıdır. Spark ise hafızası (RAM) çok kuvvetli bir matematikçi gibidir. İşlemleri sürekli kağıda yazmak (disk) yerine aklında (bellek) tutarak yapar. Bu yüzden Hadoop’un kendi işlem motoruna göre çok daha hızlıdır.

**Apache Mahout** da bu sistemin istatistikçisidir. Elinizdeki veriden geleceğe dair tahminler yapmak veya veriyi sınıflandırmak istiyorsanız (örneğin; bu müşteri ürünü alır mı almaz mı?), Mahout içindeki hazır matematiksel algoritmaları kullanırsınız.

---

Şimdi bu araçların mimari yapılarına ve teknik detaylarına biraz daha yakından bakalım.

**Apache Hive ve Veri Ambarı Yaklaşımı**
Hive, Facebook tarafından geliştirilmiş ve sonrasında açık kaynak haline getirilmiş bir veri ambarı altyapısıdır. Temel amacı, SQL bilen analistlerin Hadoop üzerinde rahatça çalışabilmesini sağlamaktır. Hive, yazdığınız HQL (Hive Query Language) sorgularını alır ve arka planda bunları MapReduce veya Tez işlerine dönüştürür.
Burada dikkat etmeniz gereken nokta şudur: Hive, gerçek zamanlı bir veritabanı değildir. OLTP (Online Transaction Processing) işlemlerinden ziyade OLAP (Online Analytical Processing) için tasarlanmıştır. Yani "bir satır sileyim, hemen güncelleyeyim"den ziyade, "son 10 yılın satış verilerini analiz edeyim" senaryoları için uygundur. Verinin şeması, veri yazılırken değil okunurken kontrol edilir (Schema on Read), bu da esneklik sağlar.

**Apache Pig ve Veri Akışı (Data Flow)**
Yahoo tarafından geliştirilen Pig, "Pig Latin" adı verilen bir betik dili kullanır. SQL bildirimseldir (ne istediğinizi söylersiniz), Pig Latin ise prosedüreldir (nasıl yapılacağını adım adım söylersiniz). Bu, özellikle yapılandırılmamış veya yarı yapılandırılmış veriler üzerinde ETL (Extract, Transform, Load) işlemleri yaparken büyük avantaj sağlar. Karmaşık MapReduce zincirlerini (Join, Group, Filter) çok daha az satır kodla yazmanıza olanak tanır.

**Apache Spark ve Bellek İçi İşleme (In-Memory Processing)**
Spark, Hadoop ekosistemindeki en önemli kırılma noktalarından biridir. MapReduce modelindeki disk I/O (girdi/çıktı) darboğazını aşmak için RDD (Resilient Distributed Datasets) yapısını kullanır. Veriyi belleğe yükler ve iş bitene kadar orada tutar. Bu, iteratif algoritmalar (örneğin makine öğrenmesi) için performansı 100 kata kadar artırabilir.
Spark sadece hızlı değildir; aynı zamanda Spark SQL, Spark Streaming, MLlib (Makine Öğrenmesi) ve GraphX (Çizge İşleme) gibi modülleri tek bir çatı altında toplar. Yani hem veriyi işleyip hem de üzerinde makine öğrenmesi modelini aynı platformda koşturabilirsiniz.

**Apache Mahout ve Makine Öğrenmesi**
Mahout, ölçeklenebilir makine öğrenmesi kütüphanesidir. Öneri sistemleri (Recommendation), Kümeleme (Clustering) ve Sınıflandırma (Classification) algoritmalarını içerir. Gençler, burada bir parantez açmak gerekir; Mahout ilk çıktığında MapReduce üzerinde çalışıyordu ancak MapReduce'un yavaşlığı makine öğrenmesi eğitim süreçlerini hantallaştırdığı için, günümüzde Mahout daha çok Spark veya Flink gibi motorlar üzerinde çalışacak şekilde evrilmiştir veya yerini Spark'ın kendi kütüphanesi olan MLlib'e bırakmaktadır. Ancak tarihsel gelişimi ve temel algoritmaları anlamak adına bilinmesi gerekir.

**Tamamlayıcı Araçlar: Zookeeper, Sqoop ve Flume**
Bu ekosistemi ayakta tutan gizli kahramanlar da vardır:

*   **Zookeeper:** Dağıtık sistemlerde koordinasyonu sağlar. Hangi sunucu ayakta, hangisi lider, konfigürasyonlar nerede tutuluyor gibi soruların cevabı Zookeeper'dadır. Bir nevi sistemin trafik polisidir.
*   **Sqoop:** İlişkisel veritabanları (MySQL, Oracle vb.) ile Hadoop arasında veri transferi sağlar. Yapısal veriyi Hadoop'a aktarmak için kullanılır.
*   **Flume:** Log dosyaları, sosyal medya akışları gibi akan (streaming) verileri toplayıp HDFS'e aktarmak için tasarlanmıştır.

Özetle, Hadoop bir temeldir. Hive ve Pig bu temel üzerinde veri analizini kolaylaştırır, Spark hızı ve yetenekleri artırır, Mahout ise veriden anlamlı modeller çıkarmanızı sağlar.

Geçen dersimizde Hadoop ekosisteminin teorik çerçevesini ve bu ekosistemi oluşturan temel oyuncuları konuştuk. Bugün, bu araçların "kaputunun altına" bakacağız. Çünkü bir sistemi sadece tanımak yetmez; onun nasıl yapılandırıldığını ve bileşenlerin birbirleriyle nasıl konuştuğunu anlamanız gerekir.

Gençler, kurulumu tamamlanmış bir Hadoop kümesinin önünde oturduğunuzda, işletim sisteminin bu araçları tanıması için yapmanız gereken ilk iş, ortam değişkenlerini (Environment Variables) tanıtmaktır.

Bir bilgisayara "Hadoop çalıştır" veya "Spark'ı başlat" dediğinizde, bilgisayarın bu komutların hangi klasörde durduğunu bilmesi gerekir. Linux tabanlı sistemlerde `.bashrc` dosyasını düzenleyerek `JAVA_HOME`, `HADOOP_HOME`, `SPARK_HOME` ve `HIVE_HOME` gibi yolları tanımlamamızın sebebi budur. Bu tanımları yapmazsanız, sistem her seferinde size "Ben bu komutu tanımıyorum" cevabını verecektir. Bu, işin alfabesidir.

**Konfigürasyon Dosyalarının Mantığı**

Hadoop ve üzerine kurulu araçların beyni, `conf` veya `etc` klasörleri altındaki XML dosyalarıdır. Kurulum aşamasında sıkça karşılaşacağınız `core-site.xml`, `hdfs-site.xml`, `yarn-site.xml` ve `mapred-site.xml` dosyaları, sistemin anayasasıdır.

Örneğin, `core-site.xml` içinde NameNode'un hangi adreste çalıştığını belirtirsiniz. Eğer Hive veya Spark kullanacaksanız, bu araçlar veriyi nereden okuyacaklarını bilmek zorundadır. İşte bu XML dosyaları, Spark ve Hive'a "Veri HDFS üzerinde şu adreste duruyor, git oradan oku" talimatını veren yerdir. Bu dosyaları doğru yapılandırmadan hiçbir analiz aracı veriye ulaşamaz.

**Hive Metastore Yapısı**

Hive özelinde konuşacak olursak, en kritik kavram "Metastore"dur. Geçen hafta Hive'ın veriyi SQL benzeri dille sorguladığını söyledim. Peki, Hive tabloların isimlerini, sütun tiplerini veya verinin HDFS'teki yerini nerede tutuyor?

Verinin kendisi HDFS'tedir, evet. Ancak verinin "kimliği" (metadata) dediğimiz şema bilgisi, ilişkisel bir veritabanında tutulmak zorundadır. Varsayılan kurulumda Hive, bu iş için Derby adında küçük bir veritabanı kullanır. Ancak Derby tek kullanıcılıdır; yani aynı anda sadece bir kişi sorgu atabilir. Gerçek dünyada biz `hive-site.xml` dosyasını düzenleyerek Hive'ı MySQL veya PostgreSQL gibi daha güçlü bir veritabanına bağlarız. Buna "Remote Metastore" kurulumu diyoruz. Böylece siz veriyi sorgularken Hive önce MySQL'e gidip "Bu tablo nerede?" diye sorar, aldığı cevaba göre HDFS'ten veriyi çeker. Bu ayrımı iyi anlamanız gerekiyor: Veri HDFS'te, verinin bilgisi Metastore'da saklanır.

**Spark ve YARN İlişkisi**

Spark'a geçtiğimizde ise kaynak yönetimi devreye girer. Spark kendi başına bir "Standalone" modda çalışabilir ancak kurumsal bir yapıda Spark işlerini genellikle YARN (Yet Another Resource Negotiator) üzerinden yönetiriz.

Buradaki işleyiş şöyledir: Siz bir Spark kodu yazdınız ve `spark-submit` komutuyla bunu çalıştırdınız. Bu komut, YARN'ın kapısını çalar ve "Bana bu işlemi yapmak için 10 tane işlemci ve 50 GB RAM lazım" der. YARN, kümedeki müsait bilgisayarları (Node'ları) kontrol eder ve Spark'a gerekli kaynağı tahsis eder.

Spark mimarisinde iki temel bileşen göreceksiniz: **Driver** ve **Executors**.
Driver, yazdığınız kodun ana komuta merkezidir; işi planlar. Executor'lar ise işi fiilen yapan işçilerdir. Konfigürasyon dosyalarında (`spark-defaults.conf`) veya komut satırında belirlediğiniz `--num-executors`, `--executor-memory` gibi parametreler, işinizin ne kadar hızlı biteceğini doğrudan etkiler. Eğer kümenizin kapasitesinden fazla kaynak isterseniz işiniz kuyrukta bekler; az isterseniz de sistem kaynakları boşa harcanmış olur.

**Entegrasyon Noktaları**

Son olarak, bu araçların birbirine nasıl bağlandığına değinelim.
Spark, Hive tablolarını doğrudan okuyabilir. Ancak bunun için Spark'ın, Hive'ın konfigürasyon dosyalarına (`hive-site.xml`) erişimi olması gerekir. Genellikle bu dosyayı Spark'ın konfigürasyon klasörüne kopyalayarak bu sorunu çözeriz. Böylece Spark SQL kullanarak, sanki Hive içindeymiş gibi sorgular atabilir ancak Spark'ın bellek içi (in-memory) hızından faydalanabilirsiniz.

Özetle gençler; bugünkü teknik altyapı anlatımımızda şunu görmenizi istedim: Bu araçlar birbirinden bağımsız adalar değildir. Hepsi aynı dosya sistemini (HDFS) ve aynı kaynak yöneticisini (YARN) paylaşan, birbirlerinin konfigürasyon dosyalarını okuyarak haberleşen entegre bir yapıdır. Bir sonraki dersimizde artık terminali açıp bu konfigürasyonların pratikte nasıl işlediğini ve ilk sorgularımızı nasıl çalıştıracağımızı göreceğiz.

***
## Virtual Box ve Hadoop, Spark vd. Araçların Kurulumu 
Gençler, bugün büyük veri ekosistemini üzerine inşa edeceğimiz temeli atıyoruz. Geçen derslerde bahsettiğimiz o karmaşık mimarilerin, NameNode’ların, Spark Executor’ların çalışacağı "evi" inşa edeceğiz.

Mühendislikte bir sistemi öğrenmenin en güvenli yolu, onu izole edilmiş bir ortamda test etmektir. Kendi ana bilgisayarınızı bozmadan, hata yapmaktan korkmayacağınız bir alan yaratmak zorundasınız. İşte bu yüzden bugün **Sanallaştırma (Virtualization)** yapacağız.

Elinizdeki bilgisayarın işletim sistemi Windows, macOS veya halihazırda Linux olabilir. Bizim amacımız, bu işletim sisteminin içinde, sanal bir bilgisayar oluşturmak ve oraya **Ubuntu Linux** kurmaktır. Neden Ubuntu? Çünkü topluluk desteği en geniş, dokümantasyonu en bol ve Hadoop ekosistemiyle en uyumlu dağıtımlardan biridir.

Süreci iki ana başlıkta ele alacağız: Hazırlık ve Kurulum Mantığı.

---

### 1. Hazırlık Aşaması: Gerekli Malzemeler

İşe başlamadan önce "mutfağımızda" olması gereken iki temel dosya var. Bunları indirmelisiniz:

1.  **Sanallaştırma Yazılımı (Hypervisor):** Bilgisayarınızın donanımını (RAM, İşlemci, Disk) paylaştıracak olan yazılımdır. Biz **Oracle VirtualBox** kullanacağız. Ücretsizdir, stabildir ve eğitim için idealdir. Kendi işletim sisteminize uygun olan sürümü indirip kurun.
2.  **İşletim Sistemi İmajı (ISO Dosyası):** Sanal makineye takacağımız "sanal DVD"dir. Ubuntu’nun web sitesinden "Desktop" sürümünü indireceğiz. Burada dikkat: **LTS (Long Term Support)** ibaresi olan sürümü indirin (örneğin 20.04 LTS veya 22.04 LTS). Ara sürümler (örneğin 23.10) daha yeni özellikler barındırsa da, kararlılık bizim için yenilikten daha önemlidir.

**Kritik Uyarı: BIOS/UEFI Ayarları**
Özellikle Windows kullanan arkadaşlar; VirtualBox’ı kurup "Başlat"a bastığınızda hata alırsanız, %90 ihtimalle bilgisayarınızın BIOS ayarlarında **Sanallaştırma Teknolojisi (Intel VT-x veya AMD-V)** kapalıdır. Bilgisayarınızı yeniden başlatıp BIOS ekranına girerek bu özelliği "Enabled" (Aktif) duruma getirmeniz gerekir. Bu ayar açık olmadan 64-bit sanal makine çalıştıramazsınız.

---

### 2. Sanal Makinenin Oluşturulması

VirtualBox’ı açtığınızda "Yeni" butonuna basarak süreci başlatıyoruz. Burada rastgele "İleri" tuşuna basmak yerine, ne yaptığımızı anlayarak ilerleyelim:

*   **Kaynak Tahsisi (RAM ve İşlemci):** Hadoop ve Spark bellek sever. Ancak ana makinenizi de nefessiz bırakmamalısınız. Eğer bilgisayarınızda 16 GB RAM varsa, sanal makineye 4 GB veya 6 GB verebilirsiniz. 8 GB RAM’iniz varsa, 4 GB sınırını zorlamayın, sisteminiz donar. İşlemci çekirdeği olarak ise en az 2 çekirdek vermeye çalışın.
*   **Sanal Disk:** "VDI" formatını seçin ve diski "Değişken Olarak Ayrılan" (Dynamically Allocated) olarak ayarlayın. Bu şu demektir: Siz 50 GB alan ayırsanız bile, Ubuntu sadece kullandığı kadar yer kaplar (örneğin ilk başta 10 GB), ihtiyaç oldukça 50 GB'a kadar genişler. Disk boyutunu en az 40-50 GB yapın; büyük veri araçları yer kaplar.

### 3. Ubuntu Kurulumu

Sanal makineyi oluşturduktan sonra "Ayarlar > Depolama" kısmından indirdiğimiz ISO dosyasını sanal CD sürücüsüne takıyoruz ve makineyi başlatıyoruz.

Karşınıza klasik bir kurulum ekranı gelecek. Burada dikkat etmeniz gerekenler:

*   **Dil Seçimi:** İngilizce kurmanızı tavsiye ederim. Hata aldığınızda internette yapacağınız aramalarda İngilizce hata mesajları size çok daha doğru sonuçlar verecektir.
*   **Updates and Other Software:** "Minimal Installation" yerine "Normal Installation" seçebilirsiniz, ancak disk alanınız kısıtlıysa minimal de iş görür. "Download updates while installing Ubuntu" seçeneğini işaretleyin, güncel başlayalım.
*   **Disk Yapılandırması:** Sanal makine içinde olduğumuz için "Erase disk and install Ubuntu" seçeneğinden korkmayın. Bu işlem Windows'taki dosyalarınızı silmez; sadece sanal makineye ayırdığınız o sanal diski formatlar.
*   **Kullanıcı Adı:** Basit, hatırlaması kolay ve Türkçe karakter içermeyen bir kullanıcı adı belirleyin (örneğin: `hadoopuser` veya kendi isminiz). Şifreniz de eğitim süresince pratik olması açısından kısa olabilir ancak üretim ortamlarında güçlü şifreler kullanmanız gerektiğini unutmayın.

---

### Mevcut Linux Kullanıcıları İçin Not

Halihazırda bilgisayarında Ubuntu veya başka bir Linux dağıtımı kurulu olan arkadaşlar; sizler bir adım öndesiniz. Ancak size tavsiyem, yine de VirtualBox üzerine temiz bir kurulum yapmanızdır. Hadoop konfigürasyonları sırasında sistem dosyalarında değişiklikler yapacağız (`/etc/hosts`, `.bashrc` gibi). Kendi ana sisteminizi kirletmek veya yanlışlıkla bozmak istemezsiniz. "Sandbox" (Kum havuzu) mantığıyla, izole bir ortamda çalışmak her zaman daha güvenlidir.

### Kurulum Sonrası: İlk Temas ve "Snapshot"

Kurulum bitti, sanal makine yeniden başladı ve karşınızda Ubuntu masaüstü duruyor. Yapacağınız ilk iş şu olmalı:

1.  Sol üstteki "Activities"e tıklayın ve `Terminal` yazıp o siyah pencereyi açın.
2.  `sudo apt update` yazıp enter'a basın ve şifrenizi girin. Bu komut, sistemin paket listesini günceller.
3.  **Hayat Kurtaran Hamle: Snapshot (Anlık Görüntü)**
    İşlemleri yapmadan önce VirtualBox ana menüsüne dönün. Makineniz çalışır durumdayken veya kapalıyken bir "Snapshot" alın ve ismine "Temiz Kurulum" deyin.
    Bunun anlamı şudur: İlerleyen derslerde bir konfigürasyonu yanlış yaparsanız, sistemi komple bozarasanız; tek bir tuşla bu "Temiz Kurulum" anına, yani bugüne geri dönebilirsiniz. Bu özellik, öğrenme sürecindeki en büyük sigortanızdır.

Gençler, önümüzdeki derste bu terminal penceresini sıkça kullanacağız. Siyah ekrandan korkmayın; o ekran sizin sisteme hükmettiğiniz yerdir. Bir sonraki derste Java kurulumu ve SSH ayarlarıyla devam edeceğiz. Hazırlıklarınızı tamamlayın.

Gençler, sanal makinelerinizin hazır olduğunu varsayarak derse başlıyorum. Önünüzde temiz bir Ubuntu masaüstü ve açılmayı bekleyen bir terminal penceresi var.

Bugün yapacağımız işlem, boş bir odayı (işletim sistemi) bir atölyeye (geliştirme ortamı) dönüştürmektir. Hadoop ve diğer büyük veri araçlarının çalışabilmesi için işletim sistemine iki temel yetenek kazandırmamız gerekiyor:
1.  **Java Dili:** Hadoop’un ana dili.
2.  **Parolasız Erişim (SSH):** Hadoop’un kendi bileşenleri arasında engelsiz konuşabilmesi.

Terminalinizi açın ve adımları mantığını kavrayarak takip edin.

### 1. Sistemin Güncellenmesi

Bir inşaata başlamadan önce zemini temizlemek gerekir. Linux dünyasında "repo" dediğimiz paket listelerini güncelleyerek işe başlarız. Bu, bilgisayarınızın en son yazılım sürümlerinin nerede olduğunu öğrenmesini sağlar.

```bash
sudo apt update
sudo apt install pdsh
```

*(Not: `pdsh` yani Parallel Distributed Shell, Hadoop'un birden çok makineye aynı anda komut göndermesi için gereklidir, şimdiden kuralım.)*

### 2. Java Kurulumu (Motoru Takmak)

Daha önce de bahsettim, Hadoop Java ile yazılmıştır. Ancak her Java sürümüyle anlaşamaz. Endüstride ve eğitim materyallerinde en kararlı çalışan sürüm genellikle **Java 8**'dir. Daha yeni sürümler (Java 11, 17 vb.) bazı uyumluluk sorunları çıkarabilir. Bu derste risk almayacağız ve Java 8 (OpenJDK 8) kuracağız.

Terminale şu komutu girin:

```bash
sudo apt install openjdk-8-jdk -y
```

Kurulum bittiğinde, sistemin Java'yı gerçekten tanıyıp tanımadığını şu komutla kontrol ederiz:

```bash
java -version
```

Çıktıda `openjdk version "1.8.0_..."` görüyorsanız, motoru başarıyla taktınız demektir.

### 3. SSH Kurulumu ve Parolasız Geçiş (Anahtarları Teslim Etmek)

Geldik en çok hata yapılan yere. Burayı dikkatli dinleyin.

Hadoop çalıştığında, **NameNode** (Yönetici), **DataNode** (İşçi) ile iletişim kurmak ister. Biz şu an tek bir bilgisayar kullanıyor olsak da (Pseudo-Distributed), Hadoop bunu bilmez; ağ üzerinden başka bir makineye bağlanıyormuş gibi davranır. Bu bağlantı sırasında sürekli "Parola nedir?" diye sormaması gerekir.

Bunun için **SSH (Secure Shell)** protokolünü yapılandıracağız.

**Adım 3.1: SSH Sunucusunu Kurun**
Ubuntu'nun masaüstü sürümünde SSH sunucusu varsayılan olarak gelmeyebilir. Önce onu kuralım:

```bash
sudo apt install openssh-server -y
```

**Adım 3.2: Anahtar Çifti Oluşturma (Key Generation)**
Şimdi dijital bir kimlik kartı oluşturacağız. Bu karta sahip olan kişi (yani yine siz), sisteme parolasız girebilecek.

Şu komutu yazın:

```bash
ssh-keygen -t rsa -P ""
```

*   Bize "Dosyayı nereye kaydedeyim?" diye soracak, **Enter**'a basıp geçin (varsayılan yer iyidir).
*   `-P ""` parametresi, parolasız bir anahtar istediğimizi belirtir. Eğer burayı boş bırakmazsanız, Hadoop her çalıştığında sizden bu parolayı ister, amacımıza ulaşamayız.

**Adım 3.3: Anahtarı Tanıtma**
Oluşturduğumuz bu anahtarı (kimlik kartını), bilgisayarın "Güvenilir Kişiler Listesi"ne (authorized_keys) eklememiz lazım.

```bash
cat ~/.ssh/id_rsa.pub >> ~/.ssh/authorized_keys
```

**Adım 3.4: Test Etme**
Burası "turnusol kağıdı"dır. Eğer bu adım çalışmazsa Hadoop çalışmaz.
Terminale şu komutu yazın:

```bash
ssh localhost
```

İlk defa bağlantı kurduğunuz için size "Bu adrese güveniyor musun? (yes/no)" diye sorabilir. `yes` yazıp Enter'a basın.

Eğer sistem sizden **parola istemeden** bir alt satıra geçip yeni bir giriş yaptıysa tebrikler, SSH tünelini açtınız. Eğer hala parola istiyorsa, yukarıdaki adımları (özellikle `authorized_keys` kısmını) tekrar etmeniz gerekir. Bağlantıdan çıkmak için `exit` yazın.

### 4. Ortam Değişkenleri (Environment Variables)

Son olarak, Linux'a Java'nın nerede kurulu olduğunu "resmi olarak" bildirmemiz gerekiyor. Bunu `.bashrc` dosyasına yazacağız. Bu dosya, terminal her açıldığında çalışan ayar dosyasıdır.

Dosyayı düzenlemek için `nano` editörünü kullanalım:

```bash
nano ~/.bashrc
```

Dosyanın en altına inin (yön tuşlarıyla) ve şu satırı ekleyin:

```bash
export JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64
```
*(Not: Genellikle Java yolu budur, ama farklıysa kontrol etmek gerekir. Şimdilik standart kurulumda bu yol geçerlidir.)*

Dosyayı kaydetmek için `CTRL + O`, sonra `Enter`, çıkmak için `CTRL + X` tuşlarına basın.

Yaptığımız değişikliğin hemen aktif olması için şu komutu çalıştırın:

```bash
source ~/.bashrc
```

Kontrol etmek için:
```bash
echo $JAVA_HOME
```
Ekrana az önce yazdığımız dosya yolunu basıyorsa işlem tamamdır.

---

Gençler, şu an elimizde;
1.  Sanal makinesi çalışan,
2.  Java motoru takılmış,
3.  Kendi kendine parolasız bağlanabilen (SSH),
4.  Gerekli yol tanımları yapılmış bir sistem var.

Burası işin "mutfağıydı". Bir sonraki dersimizde asıl malzemeyi, yani **Apache Hadoop**'u indirip, bu hazırladığımız altyapının üzerine kuracağız. Terminal pencerelerinizi kapatmadan önce `sudo shutdown now` diyerek makinenizi usulüne uygun kapatmayı unutmayın.

Gençler, mutfağı hazırladık, ocağı yaktık. Şimdi sıra ana yemeği yapmaya geldi. Bugün Apache Hadoop’u indirip, sistemin beyni olan konfigürasyon dosyalarını düzenleyeceğiz.

Bu ders, sürecin en dikkat gerektiren kısmıdır. Bir XML dosyasında unutulan küçücük bir kapatma etiketi `</property>` veya yanlış yazılan bir dosya yolu, sistemin başlamamasına neden olur. O yüzden "kopyala-yapıştır" yaparken bile neyi, nereye ve neden yapıştırdığınızı sorgulayın.

### 1. Hadoop’un İndirilmesi ve Yerleşimi

Hadoop, "açık kaynak" dünyasında bir `.tar.gz` dosyası olarak dağıtılır. Bu, Windows’taki `.zip` veya `.rar` dosyalarının Linux dünyasındaki karşılığıdır.

Terminali açın. Önce dosyayı indirelim, ardından sıkıştırılmış paketi açalım. Biz bu derste Hadoop’un 3.x sürümünü (örneğin 3.3.6) kullanacağız.

```bash
# Dosyayı indirme (Link zamanla değişebilir, güncel mirror'dan aldığımızı varsayalım)
wget https://dlcdn.apache.org/hadoop/common/hadoop-3.3.6/hadoop-3.3.6.tar.gz

# Paketi açma (tar: tape archive, x: extract, z: gzip, v: verbose/göster, f: file)
tar -xzvf hadoop-3.3.6.tar.gz
```

Dosyalar çıktıktan sonra, klasör ismi çok uzun olacağı için (hadoop-3.3.6), bunu daha sade bir hale getirelim ve çalışma dizinimizi belirleyelim.

```bash
mv hadoop-3.3.6 hadoop
```

Artık ev dizininizde (`/home/kullanici_adiniz/hadoop`) Hadoop dosyaları duruyor.

### 2. Ortam Değişkenlerinin Güncellenmesi

Linux'un bu yeni klasörden haberdar olması lazım. Geçen ders `JAVA_HOME` eklediğimiz `.bashrc` dosyasına şimdi Hadoop yollarını ekleyeceğiz.

```bash
nano ~/.bashrc
```

Dosyanın en altına şu satırları ekleyin:

```bash
export HADOOP_HOME=~/hadoop
export PATH=$PATH:$HADOOP_HOME/bin:$HADOOP_HOME/sbin
export HADOOP_CONF_DIR=$HADOOP_HOME/etc/hadoop
export HADOOP_HDFS_HOME=$HADOOP_HOME
export HADOOP_MAPRED_HOME=$HADOOP_HOME
export HADOOP_YARN_HOME=$HADOOP_HOME
```

Kaydedip çıkın (`Ctrl+O`, `Enter`, `Ctrl+X`) ve ayarları aktif edin:
```bash
source ~/.bashrc
```

### 3. Konfigürasyon Dosyalarının Düzenlenmesi

Hadoop’un ayar dosyaları `~/hadoop/etc/hadoop` klasörü altındadır. Buradaki 4 temel dosyayı düzenleyeceğiz.

**3.1. hadoop-env.sh (Java Tanıtımı)**
Hadoop bazen sistemdeki global Java yolunu bulmakta zorlanabilir. İşi garantiye almak için bu dosyanın içine Java yolunu elle yazacağız.

```bash
nano $HADOOP_HOME/etc/hadoop/hadoop-env.sh
```
Dosya içinde `export JAVA_HOME=` satırını bulun ve geçen ders öğrendiğimiz yolu yapıştırın:
`export JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64`

**3.2. core-site.xml (Adres Beyanı)**
Bu dosya, Hadoop dosya sisteminin (HDFS) hangi adreste çalışacağını belirtir.

```bash
nano $HADOOP_HOME/etc/hadoop/core-site.xml
```

`<configuration>` ve `</configuration>` etiketleri arasına şunu yazın:

```xml
<configuration>
    <property>
        <name>fs.defaultFS</name>
        <value>hdfs://localhost:9000</value>
    </property>
</configuration>
```
*Anlamı: NameNode, localhost üzerinde 9000 portunu dinleyecek.*

**3.3. hdfs-site.xml (Replikasyon Ayarı)**
Burası kritik. Hadoop varsayılan olarak her veriyi 3 farklı makineye kopyalar (Replication Factor: 3). Ancak bizim sadece 1 makinemiz var. Eğer bunu 1'e düşürmezsek, Hadoop sürekli "Diğer 2 makine nerede?" diye hata arar.

Ayrıca NameNode ve DataNode verilerinin fiziksel olarak diskinizde nereye kaydedileceğini de burada belirtmeliyiz. Önce bu klasörleri oluşturalım:

```bash
mkdir -p ~/hadoop_data/namenode
mkdir -p ~/hadoop_data/datanode
```

Şimdi dosyayı düzenleyelim:
```bash
nano $HADOOP_HOME/etc/hadoop/hdfs-site.xml
```

```xml
<configuration>
    <property>
        <name>dfs.replication</name>
        <value>1</value>
    </property>
    <property>
        <name>dfs.namenode.name.dir</name>
        <value>file:///home/KULLANICI_ADINIZ/hadoop_data/namenode</value>
    </property>
    <property>
        <name>dfs.datanode.data.dir</name>
        <value>file:///home/KULLANICI_ADINIZ/hadoop_data/datanode</value>
    </property>
</configuration>
```
*(Not: KULLANICI_ADINIZ kısmını kendi kullanıcı adınızla değiştirmeyi unutmayın.)*

**3.4. mapred-site.xml ve yarn-site.xml (İşlem Gücü)**
Hadoop sadece depolama değil, işlemedir. YARN ayarlarını yaparak MapReduce işlerinin yönetilmesini sağlarız.

`mapred-site.xml`:
```xml
<configuration>
    <property>
        <name>mapreduce.framework.name</name>
        <value>yarn</value>
    </property>
</configuration>
```

`yarn-site.xml`:
```xml
<configuration>
    <property>
        <name>yarn.nodemanager.aux-services</name>
        <value>mapreduce_shuffle</value>
    </property>
</configuration>
```

### 4. NameNode Formatlama (İlk Kurulum)

Tüm ayarlar bitti. Şimdi HDFS dosya sistemini, tıpkı yeni alınmış bir USB belleği formatlar gibi formatlamamız gerekiyor.
**Dikkat:** Bu işlemi **sadece bir kez**, ilk kurulumda yaparsınız. Eğer sistem doluyken yaparsanız tüm veriniz silinir.

```bash
hdfs namenode -format
```

Ekranda akan yazılar arasında "Storage directory has been successfully formatted" ifadesini görüyorsanız işlem başarılıdır. Eğer "Error" veya "Exception" görüyorsanız, XML dosyalarında yazım hatası yapmışsınızdır.

### 5. Sistemi Başlatma ve Kontrol (JPS)

Artık marşa basabiliriz.

HDFS'i başlatmak için:
```bash
start-dfs.sh
```

YARN'ı başlatmak için:
```bash
start-yarn.sh
```

Sistemin çalışıp çalışmadığını anlamanın en kesin yolu `jps` (Java Process Status) komutudur. Terminale `jps` yazın. Görmeniz gereken liste şudur:

*   NameNode
*   DataNode
*   ResourceManager
*   NodeManager
*   SecondaryNameNode
*   Jps

Eğer bunlardan biri (örneğin DataNode) eksikse, o servisin log dosyasına bakarak neden başlamadığını araştırmamız gerekir.

Gençler, eğer `jps` çıktınız tam ise, tarayıcınızı açıp `http://localhost:9870` adresine gidin. Karşınızda Hadoop’un yönetim paneli (Dashboard) belirecektir. Bu ekranı görebiliyorsanız, tebrikler; artık çalışan bir Büyük Veri kümeniz var. Bir sonraki derste bu boş kümeye ilk verimizi yükleyip analiz edeceğiz.

***
## Uygulama: İlk Hadoop İşimiz - WordCount

Gençler, geçen hafta zorlu bir kurulum sürecini atlattınız. `jps` komutunu yazdığınızda o listeyi görmek, sistemin kalbinin attığını gösterir. Ancak şu an elinizde boş bir fabrika var. Makineler çalışıyor ama bantların üzerinde işlenecek hammadde yok.

Bugün fabrikayı üretime geçireceğiz. HDFS (Hadoop Distributed File System) ile konuşmayı öğrenecek, ilk verimizi sisteme yükleyecek ve büyük veri dünyasının "Merhaba Dünya"sı (Hello World) kabul edilen **WordCount** (Kelime Sayma) uygulamasını çalıştıracağız.

Terminal pencerelerinizi açın, başlıyoruz.

### 1. Dosya Sistemi Ayrımı: Linux vs. HDFS

Önce bir kavram kargaşasını önleyelim. Şu an karşınızdaki Ubuntu'nun bir dosya sistemi var (Ext4). Bir de Hadoop'un kendi içindeki sanal dosya sistemi (HDFS) var.

Masaüstünde duran bir dosyayı Hadoop doğrudan göremez. Tıpkı bilgisayarınızdaki bir fotoğrafı Instagram'a yüklemeden arkadaşlarınızın görememesi gibi. Önce veriyi "Local" ortamdan "HDFS" ortamına taşımanız (upload) gerekir.

Komutlarımız Linux komutlarına çok benzer, sadece başına `hdfs dfs` ekini getiririz.

### 2. HDFS Üzerinde Klasör Açma

Hadoop'ta çalışırken düzenli olmak esastır. Önce HDFS içinde kendimize bir çalışma alanı yaratalım.

```bash
hdfs dfs -mkdir /user
hdfs dfs -mkdir /user/hadoopuser
hdfs dfs -mkdir /user/hadoopuser/giris_verisi
```

Bu komutlarla HDFS'in kök dizininde hiyerarşik bir yapı oluşturduk. Linux'taki `mkdir` komutunun aynısıdır, sadece HDFS üzerinde çalışır.

### 3. Veri Hazırlığı ve Yükleme (Upload)

Şimdi analiz edeceğimiz veriyi oluşturalım. Henüz elimizde terabaytlık veriler yok, o yüzden kendi "küçük" büyük verimizi yapacağız.

Linux ortamında (HDFS'te değil) bir metin dosyası oluşturalım:

```bash
nano deneme.txt
```

İçine rastgele İngilizce cümleler yazın. Kelime sayma yapacağımız için tekrar eden kelimeler olsun:
*Hadoop is powerful. Hadoop is open source. Big Data is future. Hadoop and Spark are friends.*

Kaydedip çıkın (`Ctrl+O`, `Enter`, `Ctrl+X`).

Şimdi bu dosyayı Linux'tan alıp HDFS'teki fabrikaya sokalım:

```bash
hdfs dfs -put deneme.txt /user/hadoopuser/giris_verisi/
```

Dosyanın gidip gitmediğini kontrol edelim:

```bash
hdfs dfs -ls /user/hadoopuser/giris_verisi/
```

Listede `deneme.txt` dosyasını görüyorsanız, veri artık işlenmeye hazırdır.

### 4. İlk MapReduce İşini Çalıştırma: WordCount

Hadoop kurulduğunda içinde örnek uygulamaların olduğu bazı `.jar` dosyalarıyla gelir. Biz Java kodu yazmadan önce bu hazır kodu kullanacağız.

Bu komut biraz uzundur, mantığını anlatayım:
`hadoop jar [Jar Dosyasının Yeri] [Programın Adı] [Girdi Yeri] [Çıktı Yeri]`

Komutu dikkatlice yazın:

```bash
hadoop jar $HADOOP_HOME/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.3.6.jar wordcount /user/hadoopuser/giris_verisi /user/hadoopuser/sonuc_cikti
```
*(Not: Sürüm numaranız 3.3.6'dan farklıysa `hadoop-mapreduce-examples-*.jar` yazarak tab tuşuna basıp tamamlayabilirsiniz.)*

**Çok Kritik Bir Kural:**
Gençler, komutun sonundaki `/user/hadoopuser/sonuc_cikti` klasörü **HDFS'te var olmamalıdır**. Hadoop, yanlışlıkla mevcut bir verinin üzerine yazmamak için, çıktı klasörünü kendisi oluşturmak ister. Eğer bu klasör zaten varsa "Output directory already exists" hatası alırsınız ve program çalışmaz.

**Terminalde Neler Oluyor?**
Enter'a bastığınızda terminal akmaya başlayacak.
*   `Connecting to ResourceManager...`: İş YARN'a teslim ediliyor.
*   `map 0% reduce 0%`: İşlem başlıyor.
*   `map 100% reduce 0%`: Veriler parçalandı ve sayıldı (Mapping).
*   `map 100% reduce 100%`: Sayımlar birleştirildi (Reducing).

"Completed successfully" yazısını gördüyseniz, ilk büyük veri operasyonunuzu başarıyla tamamladınız.

### 5. Sonuçları Okuma

Hadoop sonuçları ekrana basmaz, HDFS'e dosyalar halinde yazar. Bakalım sonuç klasöründe neler var:

```bash
hdfs dfs -ls /user/hadoopuser/sonuc_cikti
```

Burada `_SUCCESS` (işin başarılı olduğunu gösteren boş dosya) ve `part-r-00000` (asıl sonuç dosyası) göreceksiniz.

Sonucu okumak için:

```bash
hdfs dfs -cat /user/hadoopuser/sonuc_cikti/part-r-00000
```

Ekranda şuna benzer bir çıktı göreceksiniz:
```text
Big     1
Data    1
Hadoop  2
Spark   1
is      3
...
```

### Özetle Ne Yaptık?
Arka planda inanılmaz bir işbirliği gerçekleşti.
1.  Siz veriyi HDFS'e attınız, **NameNode** bu verinin hangi bloklarda saklanacağını kaydetti.
2.  İşi başlattığınızda **YARN (ResourceManager)**, kümedeki kaynakları (CPU/RAM) ayarladı.
3.  Jar dosyası **NodeManager**'lara dağıtıldı.
4.  Kod verinin olduğu yere gitti (Data Locality), işlemi yaptı ve sonucu tekrar HDFS'e yazdı.

Bugünlük bu kadar. Ham veriyi yüklemeyi ve işlemeyi gördük. Ancak fark ettiyseniz `wordcount` gibi basit bir iş için bile çok uzun komutlar yazdık. Gelecek derste, bu karmaşık komutlar yerine SQL benzeri bir dil kullanarak aynı işlemleri **Apache Hive** ile nasıl çok daha kolay yapacağımızı göreceğiz. Makinelerinizi kapatabilirsiniz.
